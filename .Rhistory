dim(lda.out@beta)
terms(lda.out,12)
posterior_lda <- posterior(lda.out)
round(posterior_lda$topics,3)
########################################################
Sys.time()
library("rJava", lib.loc="~/R/win-library/3.5")
library(KoNLP)
library("KoNLP", lib.loc="~/R/win-library/3.5")
#1.공란처리 -> /n /t, 2칸떨어진것 모두 한칸으로 바꿔줌
#2.대소문자 통일 -> 의대생분과 상의 후 작업
#3.숫자표현 -> 숫자들을 모두 _number_로 바꿔 인식 or 숫자 그대로 인식 or 숫자와 뒤의 단어를 합쳐 저장 ex) 3회, 3번 등 어떻게 할 것인지
#4.문장부호 및 특수문자 제거 -> 특이 케이스만 분류해 합쳐줘 저장.  만약 그렇다면 의대생분과 작업, 아니라면 어떻게 할 것인지
#5.불용단어 제거 ->로직 생각해서 적용 한글은 필요없는 품사 제거 / (명사는 명사로 남기고 다른 품사는 또 어떻게 처리할것인지 고민)
#6.어근 동일화 처리 -> 한글은 적용X,  영어는 어떻게 할것인지
#7.엔그램(n번 연이어 등장하는 단어들의 연쇄) ->하나의 의학용어는 붙여야하는지 따로 할 것인지 붙인다면 의대생분과 상의후 작업 #방법은 지정된 영어문자로
#변경후 ex) Unique_word1 나중에 POS_EXTRACTION함수 실행하면 영어니까 그대로 나옴 그 후 list에서 찾아서 다시 바꿔줌
#8.영어 접두사, 접미사 제거해야할지?  맞다면 그것만 하면 되는지 # 일단 끝나고 해야함 지정 용어외엔 단어 정립후 앞뒤로 뗄지??
#사전 준비사항########################################
#XML_Parsing_Pro7에서 file=""에 RDS 파일 경로를 써주고 실행한 후 실행 할 것.
######################################################
Sys.time()
# load packages
if(!require(rJava)) {
install.packages('rJava')
}
if(!require(KoNLP)) {
install.packages('KoNLP')
}
if(!require(devtools)) {
install.packages('devtools')
}
#library(devtools)
#install_github('haven-jeon/NIADic/NIAdic', build_vignettes = TRUE)
if(!require(topicmodels)) {
install.packages('topicmodels')
}
if(!require(openNLP)) {
install.packages('openNLP')
}
if(!require(NLP)) {
install.packages('NLP')
}
library(KoNLP)
#library(rJava)
library(topicmodels)
library(stringr)
#search###############################################
search_df <- result_xml_df[result_xml_df$`<MN>`=='현병력',] # 태그, 검색어 지정 ex) <MN>, '약명'
######################################################
tag ='<TD>' # NLP 처리하고 싶은 tag 입력
#검색 후 tag가 NA인 행을 삭제
search_df[,tag][is.na(search_df[,tag])] <- ""
for (i in nrow(search_df):1){# 뒤에서부터 삭제해 행이 밀려서 삭제 되지 않도록 처리함.
if(search_df[i,tag] == ""){
search_df <- search_df[-i,]
}
}
#POS 추출 함수
K_POS_EXTRACTION <- function(wordlist){
#5.불용 단어 제거
wordlist <- gsub('/F+','/CW+',wordlist)
wordlist <- gsub('/NC+','/CW+',wordlist)
pos_start <- as.vector(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]]) # 정규표현식을 통해 걸러서 사용
pos_length <- as.vector(attr(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]],'match.length'))
pos_end <- pos_start+pos_length-5
#합치되 순서는 유지해야함. -> 비효율
# /의 위치를 얻어서 뒤를 삭제 -> 비효율적
word_data = rep(NA,length(pos_start))
for(i in 1:length(pos_start)){
word_data[i] <- substr(wordlist,pos_start[i],pos_end[i])
}
return(word_data)
}
#
NLP_PROCESSING <- function(xmldf){
#4.특수 문자 변경 및 제거
xmldf <- gsub('&#x0D;', " ", xmldf) # 띄어쓰기는 마지막에 무조건 하나로 통일 해주는 부분이 있음.
xmldf <- gsub('&lt;', " ", xmldf)
xmldf <- gsub('&gt;', " ", xmldf)
xmldf <- gsub('&amp;', " ", xmldf)
xmldf <- gsub('&quot;', " ", xmldf)
xmldf <- gsub("[~!@#$%^&*()]"," ", xmldf)#특수문자 제거
#2.대소문자 통일(선택가능으로 만들 것)
#xmldf<- toupper(xmldf) # 대문자
xmldf<- tolower(xmldf)# 소문자
#6.어근 동일화 처리
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' is ',' be ',xmldf)
#또는 중요하지않은 단어들을 빼고 싶다
#xmldf <- gsub('and|of|as|in',"",xmldf)# 선처리 후 공란 처리 할 것.
#7.엔그램
#xmldf <- sub('[^A-Za-z 가-힣]*graphic[ _-]variant[^A-Za-z 가-힣]*','graphicvariant',xmldf) # KoNLP 처리시 영어문장은 그대로 나오기 때문에 한단어로 바로 나옴.
#1.공란처리
xmldf <- str_replace_all(xmldf,"[[:space:]]{1,}"," ")
#바꿀 형식
xmldf <- paste(xmldf,'.',sep = '')#문장이 아닌 경우 날짜, 약명 등 중요한 정보가 잘리는 경우가 있음. ex) 12-02-02 단어 하나있으면 12-02-0 과 2로 나뉨.
#어차피 마지막 . 추가해주면 따로 나눠지고 정규표현식에서 거르지 않으니 괜찮을거라고 생각함.
}
########MAIN CODE##############################################################
#NLP 용 df 만들기
xml_df <- search_df[tag]
#NLP_PROCESSING 함수를 통한 초기 설정
word_df <- apply(xml_df,2,NLP_PROCESSING)
#형태소 분석후 합치기
result_word_list <- list()
for (i in 1:nrow(word_df)){
word_list<-SimplePos22(word_df[i])
if(length(word_list) ==1){
word_vector <- word_list[[1]]
result_word_list[[i]] <- c(word_vector)
}
else{
word_vector <- word_list[[1]]
for (k in 2:length(word_list)){
word_vector <- paste(word_vector,'+',word_list[[k]],sep = '')
}
result_word_list[[i]] <- c(word_vector)
}
}
#원하는 품사 추출
pos_word <- lapply(result_word_list,K_POS_EXTRACTION)
#단어 리스트 -> 집합
unique_pos_vector <- unique(unlist(pos_word))
unique_pos_vector
#""일경우 지워줘야함
for (NULLValue in 1:(length(unique_pos_vector))){
if (unique_pos_vector[NULLValue] == ""){
unique_pos_vector = unique_pos_vector[-NULLValue]
break
}
}
#한글 품사 추출뒤 영어 진행
#8.영어의 경우는 지정 용어외엔 단어 정립후 앞뒤로 뗄지??
###한글 품사 추출후 unique_pos_vector 설정후에 그 단어들로 추출하는게 어떨까
#library('NLP')
#library('openNLP')
#library('tm')
#unique_pos_vector <- gsub("[가-힣]+",NA,unique_pos_vector)
#unique_pos_vector
#띄어쓰기로 구분해서 합치자
#unique_pos_word <- unique_pos_vector[1]
#for (i in 2:length(unique_pos_vector)){
#    unique_pos_word <- paste(unique_pos_word,unique_pos_vector[i], sep = " ")
#}
#unique_pos_word
#sent <- annotate(unique_pos_word,Maxent_Sent_Token_Annotator())
#word <- annotate(unique_pos_word,Maxent_Word_Token_Annotator(),sent)
#PosTag <- annotate(unique_pos_word,Maxent_POS_Tag_Annotator(),word)
#PosTag
########MAIN CODE##############################################################
##추가적인 토픽모형 작업###############################
#pos_word가 비어있는 부분은 pos_word와 search_df에서도 삭제
for (i in length(pos_word):1){
if (pos_word[[i]][1] == ""){
pos_word <- pos_word[-i]
search_df <- search_df[-i,]
}
}
#DTM Matrix 만들기
DTM <- matrix(nrow=nrow(search_df),ncol=length(unique_pos_vector),data=0)
#ROW,COL 이름 주기
NOTE_ID_list <- c(search_df['NOTE_ID'])
rownames(DTM) <- NOTE_ID_list[['NOTE_ID']]
colnames(DTM) <- unique_pos_vector
dim(DTM)
#DTM에 문서에 단어 카운트 할당
for(i in 1:nrow(DTM)){
for(k in pos_word[[i]]){
DTM[i,k] <- DTM[i,k] + 1
}
}
#Topic 예측 예제
lda.out <- LDA(DTM,control = list(seed=11),k=5)
dim(lda.out@gamma)
dim(lda.out@beta)
terms(lda.out,12)
posterior_lda <- posterior(lda.out)
round(posterior_lda$topics,3)
########################################################
Sys.time()
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jdk1.8.0_171")
library(rJava)
library(KoNLP)
library(KoNLP)
#1.공란처리 -> /n /t, 2칸떨어진것 모두 한칸으로 바꿔줌
#2.대소문자 통일 -> 의대생분과 상의 후 작업
#3.숫자표현 -> 숫자들을 모두 _number_로 바꿔 인식 or 숫자 그대로 인식 or 숫자와 뒤의 단어를 합쳐 저장 ex) 3회, 3번 등 어떻게 할 것인지
#4.문장부호 및 특수문자 제거 -> 특이 케이스만 분류해 합쳐줘 저장.  만약 그렇다면 의대생분과 작업, 아니라면 어떻게 할 것인지
#5.불용단어 제거 ->로직 생각해서 적용 한글은 필요없는 품사 제거 / (명사는 명사로 남기고 다른 품사는 또 어떻게 처리할것인지 고민)
#6.어근 동일화 처리 -> 한글은 적용X,  영어는 어떻게 할것인지
#7.엔그램(n번 연이어 등장하는 단어들의 연쇄) ->하나의 의학용어는 붙여야하는지 따로 할 것인지 붙인다면 의대생분과 상의후 작업 #방법은 지정된 영어문자로
#변경후 ex) Unique_word1 나중에 POS_EXTRACTION함수 실행하면 영어니까 그대로 나옴 그 후 list에서 찾아서 다시 바꿔줌
#8.영어 접두사, 접미사 제거해야할지?  맞다면 그것만 하면 되는지 # 일단 끝나고 해야함 지정 용어외엔 단어 정립후 앞뒤로 뗄지??
#사전 준비사항########################################
#XML_Parsing_Pro7에서 file=""에 RDS 파일 경로를 써주고 실행한 후 실행 할 것.
######################################################
Sys.time()
# load packages
if(!require(rJava)) {
install.packages('rJava')
}
if(!require(KoNLP)) {
install.packages('KoNLP')
}
if(!require(devtools)) {
install.packages('devtools')
}
#library(devtools)
#install_github('haven-jeon/NIADic/NIAdic', build_vignettes = TRUE)
if(!require(topicmodels)) {
install.packages('topicmodels')
}
if(!require(openNLP)) {
install.packages('openNLP')
}
if(!require(NLP)) {
install.packages('NLP')
}
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jdk1.8.0_171")
library(rJava)
library(KoNLP)
library(topicmodels)
library(stringr)
#search###############################################
search_df <- result_xml_df[result_xml_df$`<MN>`=='현병력',] # 태그, 검색어 지정 ex) <MN>, '약명'
######################################################
tag ='<TD>' # NLP 처리하고 싶은 tag 입력
#검색 후 tag가 NA인 행을 삭제
search_df[,tag][is.na(search_df[,tag])] <- ""
for (i in nrow(search_df):1){# 뒤에서부터 삭제해 행이 밀려서 삭제 되지 않도록 처리함.
if(search_df[i,tag] == ""){
search_df <- search_df[-i,]
}
}
#POS 추출 함수
K_POS_EXTRACTION <- function(wordlist){
#5.불용 단어 제거
wordlist <- gsub('/F+','/CW+',wordlist)
wordlist <- gsub('/NC+','/CW+',wordlist)
pos_start <- as.vector(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]]) # 정규표현식을 통해 걸러서 사용
pos_length <- as.vector(attr(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]],'match.length'))
pos_end <- pos_start+pos_length-5
#합치되 순서는 유지해야함. -> 비효율
# /의 위치를 얻어서 뒤를 삭제 -> 비효율적
word_data = rep(NA,length(pos_start))
for(i in 1:length(pos_start)){
word_data[i] <- substr(wordlist,pos_start[i],pos_end[i])
}
return(word_data)
}
#
NLP_PROCESSING <- function(xmldf){
#4.특수 문자 변경 및 제거
xmldf <- gsub('&#x0D;', " ", xmldf) # 띄어쓰기는 마지막에 무조건 하나로 통일 해주는 부분이 있음.
xmldf <- gsub('&lt;', " ", xmldf)
xmldf <- gsub('&gt;', " ", xmldf)
xmldf <- gsub('&amp;', " ", xmldf)
xmldf <- gsub('&quot;', " ", xmldf)
xmldf <- gsub("[~!@#$%^&*()]"," ", xmldf)#특수문자 제거
#2.대소문자 통일(선택가능으로 만들 것)
#xmldf<- toupper(xmldf) # 대문자
xmldf<- tolower(xmldf)# 소문자
#6.어근 동일화 처리
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' is ',' be ',xmldf)
#또는 중요하지않은 단어들을 빼고 싶다
#xmldf <- gsub('and|of|as|in',"",xmldf)# 선처리 후 공란 처리 할 것.
#7.엔그램
#xmldf <- sub('[^A-Za-z 가-힣]*graphic[ _-]variant[^A-Za-z 가-힣]*','graphicvariant',xmldf) # KoNLP 처리시 영어문장은 그대로 나오기 때문에 한단어로 바로 나옴.
#1.공란처리
xmldf <- str_replace_all(xmldf,"[[:space:]]{1,}"," ")
#바꿀 형식
xmldf <- paste(xmldf,'.',sep = '')#문장이 아닌 경우 날짜, 약명 등 중요한 정보가 잘리는 경우가 있음. ex) 12-02-02 단어 하나있으면 12-02-0 과 2로 나뉨.
#어차피 마지막 . 추가해주면 따로 나눠지고 정규표현식에서 거르지 않으니 괜찮을거라고 생각함.
}
########MAIN CODE##############################################################
#NLP 용 df 만들기
xml_df <- search_df[tag]
#NLP_PROCESSING 함수를 통한 초기 설정
word_df <- apply(xml_df,2,NLP_PROCESSING)
#형태소 분석후 합치기
result_word_list <- list()
for (i in 1:nrow(word_df)){
word_list<-SimplePos22(word_df[i])
if(length(word_list) ==1){
word_vector <- word_list[[1]]
result_word_list[[i]] <- c(word_vector)
}
else{
word_vector <- word_list[[1]]
for (k in 2:length(word_list)){
word_vector <- paste(word_vector,'+',word_list[[k]],sep = '')
}
result_word_list[[i]] <- c(word_vector)
}
}
#원하는 품사 추출
pos_word <- lapply(result_word_list,K_POS_EXTRACTION)
#단어 리스트 -> 집합
unique_pos_vector <- unique(unlist(pos_word))
unique_pos_vector
#""일경우 지워줘야함
for (NULLValue in 1:(length(unique_pos_vector))){
if (unique_pos_vector[NULLValue] == ""){
unique_pos_vector = unique_pos_vector[-NULLValue]
break
}
}
#한글 품사 추출뒤 영어 진행
#8.영어의 경우는 지정 용어외엔 단어 정립후 앞뒤로 뗄지??
###한글 품사 추출후 unique_pos_vector 설정후에 그 단어들로 추출하는게 어떨까
#library('NLP')
#library('openNLP')
#library('tm')
#unique_pos_vector <- gsub("[가-힣]+",NA,unique_pos_vector)
#unique_pos_vector
#띄어쓰기로 구분해서 합치자
#unique_pos_word <- unique_pos_vector[1]
#for (i in 2:length(unique_pos_vector)){
#    unique_pos_word <- paste(unique_pos_word,unique_pos_vector[i], sep = " ")
#}
#unique_pos_word
#sent <- annotate(unique_pos_word,Maxent_Sent_Token_Annotator())
#word <- annotate(unique_pos_word,Maxent_Word_Token_Annotator(),sent)
#PosTag <- annotate(unique_pos_word,Maxent_POS_Tag_Annotator(),word)
#PosTag
########MAIN CODE##############################################################
##추가적인 토픽모형 작업###############################
#pos_word가 비어있는 부분은 pos_word와 search_df에서도 삭제
for (i in length(pos_word):1){
if (pos_word[[i]][1] == ""){
pos_word <- pos_word[-i]
search_df <- search_df[-i,]
}
}
#DTM Matrix 만들기
DTM <- matrix(nrow=nrow(search_df),ncol=length(unique_pos_vector),data=0)
#ROW,COL 이름 주기
NOTE_ID_list <- c(search_df['NOTE_ID'])
rownames(DTM) <- NOTE_ID_list[['NOTE_ID']]
colnames(DTM) <- unique_pos_vector
dim(DTM)
#DTM에 문서에 단어 카운트 할당
for(i in 1:nrow(DTM)){
for(k in pos_word[[i]]){
DTM[i,k] <- DTM[i,k] + 1
}
}
#Topic 예측 예제
lda.out <- LDA(DTM,control = list(seed=11),k=5)
dim(lda.out@gamma)
dim(lda.out@beta)
terms(lda.out,12)
posterior_lda <- posterior(lda.out)
round(posterior_lda$topics,3)
########################################################
Sys.time()
library(KoNLP)
Sys.setenv(JAVA_HOME="C:\Program Files\Java\jdk1.8.0_171\")
library(rJava)
library(KoNLP)
library(topicmodels)
library(stringr)
#search###############################################
search_df <- result_xml_df[result_xml_df$`<MN>`=='현병력',] # 태그, 검색어 지정 ex) <MN>, '약명'
######################################################
tag ='<TD>' # NLP 처리하고 싶은 tag 입력
#검색 후 tag가 NA인 행을 삭제
search_df[,tag][is.na(search_df[,tag])] <- ""
for (i in nrow(search_df):1){# 뒤에서부터 삭제해 행이 밀려서 삭제 되지 않도록 처리함.
if(search_df[i,tag] == ""){
search_df <- search_df[-i,]
}
}
#POS 추출 함수
K_POS_EXTRACTION <- function(wordlist){
#5.불용 단어 제거
wordlist <- gsub('/F+','/CW+',wordlist)
wordlist <- gsub('/NC+','/CW+',wordlist)
pos_start <- as.vector(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]]) # 정규표현식을 통해 걸러서 사용
pos_length <- as.vector(attr(gregexpr('[^+]+\\/CW[+]',wordlist)[[1]],'match.length'))
pos_end <- pos_start+pos_length-5
#합치되 순서는 유지해야함. -> 비효율
# /의 위치를 얻어서 뒤를 삭제 -> 비효율적
word_data = rep(NA,length(pos_start))
for(i in 1:length(pos_start)){
word_data[i] <- substr(wordlist,pos_start[i],pos_end[i])
}
return(word_data)
}
#
NLP_PROCESSING <- function(xmldf){
#4.특수 문자 변경 및 제거
xmldf <- gsub('&#x0D;', " ", xmldf) # 띄어쓰기는 마지막에 무조건 하나로 통일 해주는 부분이 있음.
xmldf <- gsub('&lt;', " ", xmldf)
xmldf <- gsub('&gt;', " ", xmldf)
xmldf <- gsub('&amp;', " ", xmldf)
xmldf <- gsub('&quot;', " ", xmldf)
xmldf <- gsub("[~!@#$%^&*()]"," ", xmldf)#특수문자 제거
#2.대소문자 통일(선택가능으로 만들 것)
#xmldf<- toupper(xmldf) # 대문자
xmldf<- tolower(xmldf)# 소문자
#6.어근 동일화 처리
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' are ',' be ',xmldf)
#xmldf <- gsub(' is ',' be ',xmldf)
#또는 중요하지않은 단어들을 빼고 싶다
#xmldf <- gsub('and|of|as|in',"",xmldf)# 선처리 후 공란 처리 할 것.
#7.엔그램
#xmldf <- sub('[^A-Za-z 가-힣]*graphic[ _-]variant[^A-Za-z 가-힣]*','graphicvariant',xmldf) # KoNLP 처리시 영어문장은 그대로 나오기 때문에 한단어로 바로 나옴.
#1.공란처리
xmldf <- str_replace_all(xmldf,"[[:space:]]{1,}"," ")
#바꿀 형식
xmldf <- paste(xmldf,'.',sep = '')#문장이 아닌 경우 날짜, 약명 등 중요한 정보가 잘리는 경우가 있음. ex) 12-02-02 단어 하나있으면 12-02-0 과 2로 나뉨.
#어차피 마지막 . 추가해주면 따로 나눠지고 정규표현식에서 거르지 않으니 괜찮을거라고 생각함.
}
########MAIN CODE##############################################################
#NLP 용 df 만들기
xml_df <- search_df[tag]
#NLP_PROCESSING 함수를 통한 초기 설정
word_df <- apply(xml_df,2,NLP_PROCESSING)
#형태소 분석후 합치기
result_word_list <- list()
for (i in 1:nrow(word_df)){
word_list<-SimplePos22(word_df[i])
if(length(word_list) ==1){
word_vector <- word_list[[1]]
result_word_list[[i]] <- c(word_vector)
}
else{
word_vector <- word_list[[1]]
for (k in 2:length(word_list)){
word_vector <- paste(word_vector,'+',word_list[[k]],sep = '')
}
result_word_list[[i]] <- c(word_vector)
}
}
#원하는 품사 추출
pos_word <- lapply(result_word_list,K_POS_EXTRACTION)
#단어 리스트 -> 집합
unique_pos_vector <- unique(unlist(pos_word))
unique_pos_vector
#""일경우 지워줘야함
for (NULLValue in 1:(length(unique_pos_vector))){
if (unique_pos_vector[NULLValue] == ""){
unique_pos_vector = unique_pos_vector[-NULLValue]
break
}
}
#한글 품사 추출뒤 영어 진행
#8.영어의 경우는 지정 용어외엔 단어 정립후 앞뒤로 뗄지??
###한글 품사 추출후 unique_pos_vector 설정후에 그 단어들로 추출하는게 어떨까
#library('NLP')
#library('openNLP')
#library('tm')
#unique_pos_vector <- gsub("[가-힣]+",NA,unique_pos_vector)
#unique_pos_vector
#띄어쓰기로 구분해서 합치자
#unique_pos_word <- unique_pos_vector[1]
#for (i in 2:length(unique_pos_vector)){
#    unique_pos_word <- paste(unique_pos_word,unique_pos_vector[i], sep = " ")
#}
#unique_pos_word
#sent <- annotate(unique_pos_word,Maxent_Sent_Token_Annotator())
#word <- annotate(unique_pos_word,Maxent_Word_Token_Annotator(),sent)
#PosTag <- annotate(unique_pos_word,Maxent_POS_Tag_Annotator(),word)
#PosTag
########MAIN CODE##############################################################
##추가적인 토픽모형 작업###############################
#pos_word가 비어있는 부분은 pos_word와 search_df에서도 삭제
for (i in length(pos_word):1){
if (pos_word[[i]][1] == ""){
pos_word <- pos_word[-i]
search_df <- search_df[-i,]
}
}
#DTM Matrix 만들기
DTM <- matrix(nrow=nrow(search_df),ncol=length(unique_pos_vector),data=0)
#ROW,COL 이름 주기
NOTE_ID_list <- c(search_df['NOTE_ID'])
rownames(DTM) <- NOTE_ID_list[['NOTE_ID']]
colnames(DTM) <- unique_pos_vector
dim(DTM)
#DTM에 문서에 단어 카운트 할당
for(i in 1:nrow(DTM)){
for(k in pos_word[[i]]){
DTM[i,k] <- DTM[i,k] + 1
}
}
#Topic 예측 예제
lda.out <- LDA(DTM,control = list(seed=11),k=5)
dim(lda.out@gamma)
dim(lda.out@beta)
terms(lda.out,12)
posterior_lda <- posterior(lda.out)
round(posterior_lda$topics,3)
########################################################
Sys.time()
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jdk1.8.0_171/")
library(KoNLP)
library("rJava", lib.loc="~/R/win-library/3.5")
library(KoNLP)
detach("package:rJava", unload=TRUE)
library(KoNLP,lib.loc = "C:/Program Files/Java/jdk1.8.0_171/")
library(KoNLP,lib.loc = "C:/Program Files/Java/jdk1.8.0_171/")
library("KoNLP",lib.loc = "C:/Program Files/Java/jdk1.8.0_171/")
library("KoNLP",lib.loc = "C:/Program Files/Java/jdk1.8.0_171/")
library("KoNLP",lib.loc = "C:/Users/ParkDongsu/Documents/R/win-library/3.5")
library(KoNLP)
